---
title: "Osu Data Analysis and Visualizations"
output: html_notebook
---
## Introduction
The following data analysis is about the online (FTP) rhythm-based game called [Osu!](osu.ppy.sh). In Osu!, we click circles. We click them to the rhythm of a song (or 'beat-map'). That pretty much sums up the game itself. What's interesting is that players can be ranked based on their overall performance. The best players have a higher overall performance (abbreviated as 'pp'), and we can see their names appear on the [Performance Rankings](https://osu.ppy.sh/rankings/osu/performance). I went and took the liberty of gathering as much data as I can from this list. See the folder 'Python Code' for more about how I gathered the data. 

This notebook is what I've gathered from the data. Spoiler: It isn't much. But I have some cool visualizations. 

## Coding
We will mostly use packages from the tidyverse. 
```{r}
library(readr)
library(dplyr) # ignore the warning
library(tidyr)
library(ggplot2)

lables <- c("name","performance","accuracy","PC","SS","S","A","Date")

Player1 <- read_csv("../datasets/test2.csv",col_names = lables) %>%
  select(performance,PC,SS,S,A,Date) %>%
  filter(Date>as.Date("2016-01-01")) %>%    # useless data imo
  filter(!(between(Date,as.Date("2017-08-01"),as.Date("2017-12-31")))) %>% # just filtering dirty data
  mutate(group = 'old')

Player2 <- read_csv("../datasets/test3.csv",col_names = lables) %>%
  select(performance,PC,SS,S,A,Date) %>%
  mutate(group = 'new')

Player <- rbind(Player1,Player2)
```
I have two datasets here. One dataset uses the most current information on the Performane Rankings. The other dataset are from past Performance Rankings, before late 2017. There are a handful of differences between the two datasets, so we'll cover that along our analysis. The tibble `Player` will contain both `new` and `old` values.

There are a handful of outliers in the above datasets, so we are going to filter some values. This is hindsight of the column values having a skewed Gaussian (normal/bell-curve) distribution. 
```{r}
# helper function to help with repeated code
is_in_range <- function(obj) {
  between(obj,mean(obj)-4*sd(obj),mean(obj)+4*sd(obj))
}

filtered_data <- Player %>%
  filter(
    is_in_range(PC),
    is_in_range(S),
    is_in_range(A),
    is_in_range(SS)
  ) 

#useful for some plotting
filtered_data_means <-filtered_data %>%
  group_by(group) %>%
  summarise(mean_pf = mean(performance),
            mean_pc = mean(PC),
            mean_s = mean(S),
            mean_a = mean(A),
            mean_ss = mean(SS)) %>%
  mutate(
    group = ifelse(group == 'new','new mean','old mean')
  )

# mutating tibble filtered_data_means for the upcoming graph. For faceting purposes
filtered_data_means_2 <- filtered_data_means %>%
  gather(key = key, value = value, mean_s, mean_a) %>%
  mutate(key = ifelse(key == "mean_s","S","A")) 
```
Let's start out with some basic line-graphs and histograms. We'll comapre the difference between the 'old' and the 'new' datasets. 

```{r}
ggplot(Player) +
  geom_point(mapping = aes(x = Date, y = performance, color = group)) +
  theme_bw() +
  ggtitle("Data overview")

ggplot(Player) +
  geom_point(aes(1:nrow(Player),performance,color = Date)) +
  theme_bw() +
  ggtitle("Data overview")  

Player %>%
  arrange(desc(performance)) %>%
  ggplot(mapping = aes(x = 1:nrow(Player), y = performance, color = group)) +
  geom_point() +
  theme_bw() +
  ggtitle("Data overview")
```
We can clearly see how chopped up the data is in the first scatter plot. In particular, the `old` dataset is chopped up. A complete-ish dataset from the year, say, 2016 should look like the smooth line in the red. The chopped up look is based on what was made available to us with the [Wayback Machine](https://web.archive.org/web/*/osu.ppy.sh). 

```{r}
#basic histogram. (Good). We have the mean lines included to see the differences in old and new data. 
ggplot(data = filtered_data) + 
  geom_histogram(mapping = aes(x= PC, fill = group), bins = 100) +
  geom_vline(data = filtered_data_means, mapping = aes(xintercept = mean_pc[1], color = 'new mean'), size = 1.25, linetype="twodash") +
  geom_vline(data = filtered_data_means, mapping = aes(xintercept = mean_pc[2], color = 'old mean'), size = 1.25, linetype="twodash") +
  scale_colour_manual('Lines',values = c("red","blue")) +
  theme_bw() +
  ggtitle("Distribution of Play counts") +
  labs(x = "Play Count")
```
We see that the average `PC` (play count) has increased over time time.

```{r}
#(Very good) Faceted differences between past and new data (S and A comparisons only)
fac_histo <- filtered_data %>%
  select(S,A,group) %>%
  gather(key = key, value = value, S, A) %>%
  ggplot() +
  geom_histogram(mapping = aes(x = value, fill = group), bins = 100) +
  geom_vline(data = filtered_data_means_2, mapping = aes(xintercept = value, color = group), size = 1.25, linetype="twodash") +
  scale_colour_manual('Lines',values = c("red","blue")) +
  theme_bw()

fac_histo + facet_grid(key ~ .) +
  ggtitle("Distribution of S/A map counts") +
  labs(x = "Number of * rated beatmaps")
```
This second figures is a little interesting. It seems that current players have more `A` ranked maps on average nowadays. However, past players held more `S` ranked maps than current players on average. There's also an interesting question about the ratio of `SS/S/A' ranked maps in proportion to `play_count` and `performance`. See the Discussion section.

Now, let's look at some heat maps. 
```{r}
# (Okay enough for a plot). Interesting because faceted heat maps. 
ggplot(filtered_data) +
  geom_bin2d(aes(PC,performance)) +
  scale_fill_gradient2(high = "darkblue") +
#  facet_grid(. ~ group) +  # faceting doesn't look good though
  theme_bw() + 
  ggtitle("Performance vs Play Count") +
  labs(y = "Performance")
```


```{r}
# (Questionable). I think we already pointed out how S/A differs. 
filtered_data %>%
  gather(key = key, value = value, S, A) %>%
  ggplot() +
  geom_bin2d(aes(value,PC)) +
  scale_fill_gradient2(high = "darkblue") +
  facet_grid(. ~ key) +
  theme_bw() +
  ggtitle("Play Count vs S/A Map Count") +
  labs(y = "Play Count", x = "Number of * rated beatmaps")
```
Now we'll take a closer look at performance. One thing gives us some trouble though. I am making the assumption that performance inflates over time (linearly), so it wouldn't be ideal to compare performaces at separate years. 

Consider the graph below of the top 50 players at separate points in time. 

```{r}
top_labels <- c('name', 'performance','accuracy','PC','SS','S','A','Date')
topPlayer <- read_csv("../datasets/test.csv",col_names = FALSE) 
colnames(topPlayer) <- top_labels

sorted_top_player <- topPlayer %>%
  select(performance, PC,SS,S,A, Date) %>%
  arrange(Date) %>%
  mutate(
    id = 1:800
  )

ggplot(sorted_top_player) +
  geom_point(aes(Date,performance,color = Date)) +
  theme_bw() + 
  ggtitle("Top 100 Player performance over time") +
  labs(y = "Performance")
```
Now let's look at `PC` (play_count) of the top 50 players over time. 
```{r}
ggplot(sorted_top_player) +
  geom_point(aes(Date,PC,color = Date)) + 
  theme_bw() +
  ggtitle("Top 100 Player play counts over time") +
  labs(y = "Play Count")
```
And a quick overview of the distribution of `SS`/`S`/`A` over time
```{r}
sorted_top_player %>%
  gather(key=key,value=value,SS,S,A) %>%
  ggplot() +
    geom_point(aes(Date,value, color = Date)) +
    facet_grid(. ~ key) +
    theme_bw() +
  ggtitle("Top 100 Player SS/S/A map counts over time") +
  labs(y = "Number of * rated beatmaps")
```
Okay, there are a handful of outliers, but there is no linear growth in `PC` (play count) or `SS`/`S`/`A` like we do with performance. This argument could be stronger if we had say top 500 data at separate points in time. This is a limitation of the WayBack Machine having enough archived urls. 

Let's make a linear model of performance as a function of time. 
```{r}
model <- lm(performance ~ Date,sorted_top_player)
c <- model$coefficients[2]
d <- model$coefficients[1]

# (VERY GOOD)
sorted_top_player %>%
  mutate(
    lm_performance = as.numeric(Date)*c + d
  ) %>%
  ggplot() +
  geom_point(aes(Date,performance)) + 
  geom_line(aes(Date,lm_performance, color = "trend line"), size = 2) +
  scale_color_manual(name = c("Line"), values = c("red")) +
  theme_bw() +
  ggtitle("Top 100 Player peformance over time") +
  labs(y = "Performance")
```
This is a rough linear model, but it gives us some idea how performance 'inflates' over time. 

We can now give a type of correctional term to the 'old' player data, but we'll breifly study how performance can be modeled as a function of rank. First, let's partition the data into categories. 
```{r}
data_2016 <- filtered_data %>%
  filter(between(Date,as.Date("2016-01-01"),as.Date("2017-01-01"))) %>%
  arrange(desc(performance)) %>%
  mutate(
    id = 1:n()
  )

data_2017 <- filtered_data %>%
  filter(between(Date,as.Date("2017-01-01"),as.Date("2018-01-01"))) %>%
  arrange(desc(performance)) %>%
  mutate(
    id = 1:n()
  )
  
data_2019 <- filtered_data %>%
  filter(between(Date,as.Date("2019-01-01"),as.Date("2020-01-01"))) %>%
  arrange(desc(performance)) %>%
  mutate(
    id = 1:n()
  )
```

## Modeling
Now for some quick modeling. After some observation, the log of performance can be modeled as the log of one over the rank. That is, `log(performance) ~ log(1/(id+1))`

- This discovery was based on a simple obervation: It looked like performance could be modeled as an exponential decay term. After I took the log of performance, I expected to see a linear term since `log(exp(-x)) == -x`. This is not what i found. The `log(performance)` looked like it was exponentially decaying still. Even `log(log(performance))` still had a exponential decay look to it. I later realized that taking the log of `1/x` had an exponential decay look to it. So I used `log(performance) ~ log(1/x)`. This logs at this point are only for reducing the size of the values (keeping things less than 10).

```{r}
temp_model <- lm(log(performance) ~ log(1/(id+1)) , data_2016)
temp_model$coefficients
a <- temp_model$coefficients[2] # coefficient
b <- temp_model$coefficients['(Intercept)']

temp_model_2 <- lm(log(performance) ~ log(1/(id+1)) , data_2017)
temp_model_2$coefficients
a_2 <- temp_model_2$coefficients[2] # coefficient
b_2 <- temp_model_2$coefficients['(Intercept)']

temp_model_3 <- lm(log(performance) ~ log(1/(id+1)) , data_2019)
temp_model_3$coefficients
a_3 <- temp_model_3$coefficients[2] # coefficient
b_3 <- temp_model_3$coefficients['(Intercept)']
```
The differences in the intercepts will matter a lot here.


```{r}
data_2016 %>%
  mutate(
    lm_performance = exp(a*log(1/(id+1)) + b)
  ) %>%
  ggplot() +
  geom_line(aes(id,performance, color = "Actual"), size = 1.5) +
  geom_line(aes(id,lm_performance, color = "Model"), size = 1.5) +
  scale_color_manual(name = c("Lines"), values = c("blue", "red")) +
  theme_bw() +
  ggtitle("Model for 2016 Performance Ranking") +
  labs(x = "rank", y = "Performance")

data_2017 %>%
  mutate(
    lm_performance = exp(a_2*log(1/(id+2)) + b_2) # just shifting a bit so it can start the same
  ) %>%
  ggplot() +
  geom_line(aes(id,performance, color = "Actual"), size = 1.5) +
  geom_line(aes(id,lm_performance, color = "Model"), size = 1.5) +
  scale_color_manual(name = c("Lines"), values = c("blue", "red")) +
  theme_bw() +
  ggtitle("Model for 2017 Performance Ranking") +
  labs(x = "rank", y = "Performance")

data_2019 %>%
  mutate(
    lm_performance = exp(a_3*log(1/(id+20)) + b_3) # just shifting a bit so it can start the same
  ) %>%
  ggplot() +
  geom_line(aes(id,performance, color = "Actual"), size = 1.5) +
  geom_line(aes(id,lm_performance, color = "Model"), size = 1.5) +
  scale_color_manual(name = c("Lines"), values = c("blue", "red")) +
  theme_bw() +
  ggtitle("Model for 2019 Performance Ranking") +
  labs(x = "rank", y = "Performance")
```
These are pretty good models of Performance Ranking. 

```{r}
summary_of_models <- tibble(
  id = 1:10000,
  lm_2019 = exp(a_3*log(1/(id+20)) + b_3) - 1506.42,
  lm_2017 = exp(a_2*log(1/(id+2))+b_2) + 1136.21,
  lm_2016 = exp(a*log(1/(id+1))+ b) + 2454.19
) 

#head(summary_of_models)
summary_of_models %>%
  gather(key = key, value = value, -id) %>%
  ggplot() +
  geom_line(aes(id, value, color = key), size = 1.5) +
  theme_bw() +
  ggtitle("Performance Ranking\nComparing Models w/ peak performance = 15,000pp") +
  labs(x = "rank", y = "Performance (Linear model)")
```
I think this is pretty cool, but this is very naive way of modeling `performance`. Note the shape of curves though. I believe that there is greater `performance` disparity in the top 10,000 ranking. I would have liked a decent amount of data from 2018 to confirm this.

## Modeling Part 2.
Let's model `performance` as a linear funtion of every other variable: `PC/SS/S/A/accuracy`. I don't want to include accuracy though since it's the least important factor. 

Let's look at the correlation matrices first. 
```{r}
library(reshape2)
cormat <- cor(filtered_data[,1:5])
melted_cormat <- melt(cormat)
ggplot(melted_cormat, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  ggtitle("Correlation Heatmap")
```
From the above figure

- `S` and `SS` share the strongest correlation across the board. 
- In regards to performance, `cor(performance, PC)` had the highest value. 


```{r}
temp_model_of_performance <- lm(performance ~ PC, filtered_data)
temp_model_coeff <- temp_model_of_performance$coeff
ggplot(filtered_data) +
  geom_point(aes(PC,performance), alpha = 1/5) +
  geom_abline(aes(slope = temp_model_coeff[2], intercept = temp_model_coeff[1], color = "Linear Model"), size = 2) +
  scale_color_manual(name = "Lines", values = "red") +
  theme_bw() +
  ggtitle("Performance as a Function of Play Count") +
  labs(x = "Play Count", y = "Performance")
```
The linear model above is clearly not universal. We don't have data on the ranks below #10,000. 

Now including the other variables. I didn't expect the model to be that much better though. 
```{r}
model <- lm(performance ~ PC + SS + S + A, filtered_data[,1:5])
model_coeff <- model$coefficients

filtered_data %>%
  mutate(
    model = model_coeff['PC']*PC +  model_coeff['SS']*SS + model_coeff['S']*S + model_coeff['A']*A +  model_coeff['(Intercept)']
  ) %>%
  ggplot() +
  geom_point(aes(PC,performance), alpha = 1/5) +
  geom_point(aes(PC,model, color = "Model"), size = 1) +
  scale_color_manual(name = "Lines", values = "purple") +
  theme_bw() +
  ggtitle("Performance as a linear function of PC, SS, S, and A") +
  labs(x = "Play Count", y = "Performance")
```

```{r}
facet_plot <- filtered_data %>%
  mutate(
    model = model_coeff['PC']*PC +  model_coeff['SS']*SS + model_coeff['S']*S + model_coeff['A']*A +  model_coeff['(Intercept)']
  ) %>%
  select(-Date,-group) %>% # not needed parts
  gather(key = key, value = value, SS, S, A) %>% # for faceting
  ggplot() +
  geom_point(aes(value,performance), alpha = 1/5) +
  geom_point(aes(value,model, color = "Model"), size = 1)

facet_plot + facet_grid(. ~ key) +
  scale_color_manual(name = "Lines", values = "purple") +
  theme_bw() +
  ggtitle("Performance as a linear function of PC, SS, S, and A") +
  labs(x = "Number of * rated beatmaps", y = "Performance") 
```

So assuming this a player was in the top 10,000 rankings, we can guess their `performance` based on the collective information of `PC/SS/S/A`. This is not a good general model. 



