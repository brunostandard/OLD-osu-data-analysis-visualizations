---
title: "Osu Data Analysis and Visualizations"
output: html_notebook
---
# Welcome
We begin with isntalling basic libraries and datasets
```{r}
library(readr)
library(dplyr) # ignore the warning
library(tidyr)
library(ggplot2)
lables <- c("player_id","name","rank","performance","accuracy","play_count","SS","S","A")
Player <- read_csv("../datasets/Player.csv",col_names = lables) %>%
  select(performance,play_count,accuracy,SS,S,A)
head(Player)
```
# Basic Visualizations
```{r}
# for some reason Player isn't sorted by performance yet. 
Player <- Player %>% 
  arrange(desc(performance)) %>%
  mutate(
    id = 1:nrow(Player) # which is bascially a player's rank based on performance rating
      )
ggplot(data = Player) +
  geom_point(mapping = aes(x= id, y=performance))
```
We see that performance as a function of id (rank) follows a kind of exponential decay.

```{r}
ggplot(data = Player) + 
  geom_histogram(mapping = aes(x = play_count),bins = 80)
```
```{r}
ggplot(data = Player) +
  geom_point(mapping = aes(x = id, y = play_count), alpha = 1/5)
```
We can almost see an average increase in play_count as id (rank) approaches 1. The next plot confirms this.
```{r}
Player %>%
  ggplot(mapping = aes(x = id, y = play_count, group = cut_width(id,500))) +
  geom_boxplot()
```
We see that play_count has some relation to one's performance to their play_count. That is, the median play_count increases as one's id (rank) increases. The distribution is of play_counts is like a skewed normal with mean about 78429. I do expect one's id to correlate with their play_count. 

There are some reasons for that noise. Consider the following:

1.  Players could play off-line (and not be recorded)
2.  Players could play un-ranked maps (and not be recorded)
3.  Players could have multiple accounts and or changed accounts. 
4.  Players may not often play for rankings (or "pp farming")
 
There is something else to consider besides overall play_counts: "beat-map" plays. That is, performance is also based on the number of SS, S, and A rated beat-maps. In Osu!, the main sources of performance are SS and S maps (and occasionally A maps). A player's *overall* performance is based on their top 100 maps (usually the most difficult completed beat-maps). An algorithm determines the exact performance one gets from a single map, and it takes things like accuracy, mods, and score into account. This data only has information about a player's overall performance. 
```{r} 
Player %>% 
  gather(key = key, value = value, -id, -performance, -play_count, -accuracy) %>%
  filter(value<6000) %>% # there are a handful of extreme values 
  ggplot(mapping = aes(x = value, color = key)) + 
  geom_freqpoly(bins = 80) 
```
We filtered out map counts greater than 6000. This resulted in 33 players out of the histogram (<1% of population).

The distribution of S and A maps follow a type of (skewed) normal distribution. SS maps follow a kind of exponential distribution. This is expected since SS maps are often difficult to achieve, and it's sometimes based on luck.  

The following are the 2d histograms of SS/A/S maps and performance
```{r}
Player %>%
  filter(SS<6000,S<6000,A<6000) %>%
  ggplot() +
  geom_bin2d(mapping = aes(x = performance, y = SS))

Player %>%
  filter(SS<6000,S<6000,A<6000) %>%
  ggplot() +
  geom_bin2d(mapping = aes(x = performance, y = S))

Player %>%
  filter(SS<6000,S<6000,A<6000) %>%
  ggplot() +
  geom_bin2d(mapping = aes(x = performance, y = A))
```
We investigate the correlation between SS/S/A counts and performance a bit further. We can use the cor() function, but I prefer to go through every step of the process. Doing this will help us deal with outliers. This investigation will side track to modeling a performance. 

```{r}
centered_player <- Player %>%
  mutate(
    play_count = (play_count-mean(play_count))/sqrt(var(play_count)),
    S = (S-mean(S))/sqrt(var(S)), # centering,
    SS = (SS-mean(SS))/sqrt(var(SS)), # centering
    A = (A - mean(A))/sqrt(var(A)), # centering
    accuracy = (accuracy - mean(accuracy))/sd(accuracy),
    performance = (performance - mean(performance))/sd(performance)
  ) %>%
  select(-id) # not accepting id
```
Now we can take a better look at the outliers with boxplots:
```{r}
centered_player %>%
  gather(key = key, value = value) %>%
  ggplot(aes(x = key, y = value)) +
  geom_boxplot()
```

Osu players are clearly not afraid to stand-out. Over-the-top play_counts are great, but we will consider them to be outliers. We chose a value x to be an outlier if it is not in (mean(x)-std*4.75,mean(x)+std*4.75) to be outliers. This weak requirement alone rids of 106 players (~2% of population). 

```{r}
# function to help with repeated code
is_in_range <- function(obj) {
  between(obj,mean(obj)-4.75*sd(obj),mean(obj)+4.75*sd(obj))
}

#now filtering non-centered data
filtered_data <- Player %>%
  filter(
    is_in_range(S),
    is_in_range(SS),
    is_in_range(A),
    is_in_range(performance),
    is_in_range(accuracy)
  ) 

#now centering that filtered data (a bit redundant yes)
centered_filtered_data <- filtered_data %>%
  mutate(
    play_count = (play_count-mean(play_count))/sqrt(var(play_count)),
    S = (S-mean(S))/sqrt(var(S)), # centering,
    SS = (SS-mean(SS))/sqrt(var(SS)), # centering
    A = (A - mean(A))/sqrt(var(A)), # centering
    accuracy = (accuracy - mean(accuracy))/sd(accuracy),
    performance = (performance - mean(performance))/sd(performance)
  ) %>%
  select(performance,play_count,S,SS,A,accuracy)
```
Now we want to look at the correlation matrix of the centered_filtered_data tibble.
```{r}
prf_vec <- centered_filtered_data$performance
acc_vec <- centered_filtered_data$accuracy
s_vec  <- centered_filtered_data$S
ss_vec <- centered_filtered_data$SS
a_vec <- centered_filtered_data$A
pc_vec <- centered_filtered_data$play_count

M <- cbind(prf_vec,pc_vec,acc_vec,ss_vec,s_vec,a_vec)
CM <- cor(M)
CM[1,] # in particular, we want to look at how each variable correlates to performance. 
```
In Osu!, overall accuracy has little influence in the high ranks of the game. It matters a great deal between beat-maps, but there accuracy isn't the strongest factor in the game. A correlation of -0.067 between performance and accuracy somewhat validates this suspician. To further back this argument, we briefly investigate overall accuracy. 
```{r}
Player %>%
  ggplot(mapping = aes(x = id, y = accuracy, group = cut_width(id,500))) +
  geom_boxplot() 
Player %>%
  filter(between(accuracy,95,100)) %>% # for outliers
  ggplot(mapping = aes(x = accuracy)) +
  geom_freqpoly(bins = 50) 
  
```

With information about correlation, we'll generate some plots. 

```{r}
# 1st experimental correlation in regards to performance, q = 0.34083772
ggplot(centered_filtered_data) +
  geom_point(aes(x = play_count, y = performance),alpha = 1/20) +
  geom_abline(slope = CM['pc_vec','prf_vec'],color = "red") +
  geom_abline() # for reference

# 2nd experimental correlation in regards to performance, q = 0.14758904
ggplot(centered_filtered_data) +
  geom_point(aes(x = S, y = performance),alpha = 1/20) +
  geom_abline(slope = CM['s_vec','prf_vec'],color = "red")

# last experimental correlation in regards to performance, q = -0.06737515
ggplot(centered_filtered_data) +
  geom_point(aes(x = accuracy, y = performance),alpha = 1/20) +
  geom_abline(slope = CM['acc_vec','prf_vec'],color = "red") 

```

This is interesting observation. play_count seems to be the strongest variable in someone's performance. Intuitively, this makes a lot of sense. One plays more and gets slightly better every time they play. A correlation of 0.34 isn't too impressive, but there several things to consider. Not each player plays with the intent of practicing. Everyone wants to improve and play harder songs (beat-maps), yet they do it casually. That is to be expected. We play to have fun. 

Now consider constructing a linear model that can help us better determine one's performance given they are in the top 5000. 
# Linear Models
Ultimately, we want to use all of the variables to model performance, but we begin with modeling performance with play_count.
```{r}
model1 <- lm(play_count ~ performance, data = Player) # basic linear model

ggplot(data = Player) +
  geom_point(mapping = aes(x = performance, y = play_count), alpha = 1/10) +
  geom_abline(slope = 0,intercept = mean(Player$play_count), color = "red") +
  geom_abline(slope = model1$coefficients[2], intercept = -model1$coefficients[1], color = "blue")
```

Experimental Stuff:
```{r}
model2 <- lm(performance ~ play_count + S + A, data = Player) # basic linear model
summary(model2)
plot(model2)
```

```{r}
model2 <- lm(play_count ~ performance + S, data = Player) # basic linear model
model$coefficients
#ggplot(data = Player) +
#  geom_point(mapping = aes(x = performance, y = play_count), alpha = 1/10) +
#  geom_abline(slope = 0,intercept = mean(Player$play_count), color = "red") +
#  geom_abline(slope = model1$coefficients[2], intercept = -model1$coefficients[1], color = "blue")
```

Confirming some supsicians about lm() function.
```{r}
df <- tibble(
  x = 1:20,
  y = 1.1*x+3
)
model3 <- lm(y ~ x, data = df)
model3$coefficients
```

# Discussion


# Future works
