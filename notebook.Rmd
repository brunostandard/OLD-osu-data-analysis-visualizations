---
title: "Osu Data Analysis and Visualizations"
output: html_notebook
---
<h2>Welcome.</h2>
<p>Installing some basic libraries and datasets.</p>
```{r}
library(readr)
library(dplyr) # ignore the warning
library(tidyr)
library(ggplot2)
Player <- read_csv("datasets/Player.csv",col_names = FALSE)
lables <- c("player_id","name","rank","performance","accuracy","play_count","SS","S","A")
colnames(Player) <- lables
Player <- Player %>% select(performance,accuracy,play_count,SS,S,A)
head(Player)
```
<h2>Basic Visualizations</h2>

```{r}
# for some reason Player isn't sorted by performance yet. 
Player <- Player %>% 
  arrange(desc(performance)) %>%
  mutate(
    id = 1:nrow(Player) # which is bascially a player's rank based on performance rating
      )

head(Player)

ggplot(data = Player) +
  geom_point(mapping = aes(x= id, y=performance))
```
This exponential decay would make finding a distribution for performance pretty simple. More on that later. 

```{r}
ggplot(data = Player) +
  geom_point(mapping = aes(x = id, y = play_count), alpha = 1/5)

ggplot(data = Player) + 
  geom_histogram(mapping = aes(x = play_count),bins = 80)

Player %>%
  ggplot(mapping = aes(x = id, y = play_count, group = cut_width(id,500))) +
  geom_boxplot()
```
We see that play count has some relation to one's performance to their play count. That is, the median play count increases as one's id (rank) increases. The distribution is of play counts is like a skewed normal with mean about 78429. I do expect one's id to correlate with their play count. 

There is some noise to take into account. Tat noise being the following:
 1. Players could play off-line (plays are not recorded)
 2. Players could play unranked maps (like 1)
 3. Players could have multiple accounts and or changed accounts. 
 4. Some players are crazy? (for those crazy high play counts)
 
But wait. A player's performance is also based on their number of SS, S, and A maps. 
```{r}
# not good
Player %>% 
  gather(key = key, value = value, -id, -performance, -play_count, -accuracy) %>%
  ggplot(mapping = aes(x = value, color = key)) + 
  geom_freqpoly(bins = 80)

Player %>% 
  gather(key = key, value = value, -id, -performance, -play_count, -accuracy) %>%
  filter(value<6000) %>%
  ggplot(mapping = aes(x = value, color = key)) + 
  geom_freqpoly(bins = 80) 
```
We see that there is a handful of player that are far outside the norm. That is, some players have crazy high SS, S, A map counts. We filtered out map counts greater than 6000. This resulted in 33 players out of the histogram (<1% of population).
The distribution of S and A maps follow a type of (skewed) normal distribution. SS maps follow a kind of exponential distribution. This last one is expected since SS maps are often difficult to achieve, and it's sometimes based on luck.  

In osu!, the main source of performance is SS and S maps (and occasionally A maps). A player's <b>overall</b> performance is based on their top 100 maps (usually the most difficult completed maps). An algorithm determines the exact performance one gets from a single map, and it uses things like accuracy, mods, combo length, score(?), and many other details as its parameters. This data only has a player's overall performance. 

The following are the 2d histograms of map counts and performance
```{r}
Player %>%
  filter(SS<6000,S<6000,A<6000) %>%
  ggplot() +
  geom_bin2d(mapping = aes(x = performance, y = SS))

Player %>%
  filter(SS<6000,S<6000,A<6000) %>%
  ggplot() +
  geom_bin2d(mapping = aes(x = performance, y = S))

Player %>%
  filter(SS<6000,S<6000,A<6000) %>%
  ggplot() +
  geom_bin2d(mapping = aes(x = performance, y = A))
```
We investigat the correlation between map counts and performance a bit further. We can do this immediately with cor() function, but I prefer to go through every step of the process. We will see that his helps us get rid of outliers. This investigation will side track to modeling a performance. 

```{r}
centered_player <- Player %>%
  mutate(
    play_count = (play_count-mean(play_count))/sqrt(var(play_count)),
    S = (S-mean(S))/sqrt(var(S)), # centering,
    SS = (SS-mean(SS))/sqrt(var(SS)), # centering
    A = (A - mean(A))/sqrt(var(A)), # centering
    accuracy = (accuracy - mean(accuracy))/sd(accuracy),
    performance = (performance - mean(performance))/sd(performance)
  ) %>%
  select(performance,play_count,S,SS,A,accuracy)
```

Now viewing the outliers through boxplots:
```{r}
centered_player %>%
  gather(key = key, value = value) %>%
  ggplot(aes(x = key, y = value)) +
  geom_boxplot()
```

Osu players are clearly not afraid to stand-out. Over-the-top play counts are great, but we will consider them to be outliers. We chose a value x to be an outlier if it is not in (mean(x)-std*4.75,mean(x)+std*4.75) to be outliers. This weak requirement alone rids of 106 players (~2% of population). 

```{r}
# function to help with repeated code
is_in_range <- function(obj) {
  between(obj,mean(obj)-4.75*sd(obj),mean(obj)+4.75*sd(obj))
}

#now filtering data
filtered_data <- Player %>%
  filter(
    is_in_range(S),
    is_in_range(SS),
    is_in_range(A),
    is_in_range(performance),
    is_in_range(accuracy)
  ) 
#count(filtered_data)

#now centering that filter data (a bit redundant yes)
centered_filtered_data <- filtered_data %>%
  mutate(
    play_count = (play_count-mean(play_count))/sqrt(var(play_count)),
    S = (S-mean(S))/sqrt(var(S)), # centering,
    SS = (SS-mean(SS))/sqrt(var(SS)), # centering
    A = (A - mean(A))/sqrt(var(A)), # centering
    accuracy = (accuracy - mean(accuracy))/sd(accuracy),
    performance = (performance - mean(performance))/sd(performance)
  ) %>%
  select(performance,play_count,S,SS,A,accuracy)
```


Now we want to look at the correlation matrix of all the columns. 
```{r}
prf_vec <- centered_filtered_data$performance
acc_vec <- centered_filtered_data$accuracy
s_vec  <- centered_filtered_data$S
ss_vec <- centered_filtered_data$SS
a_vec <- centered_filtered_data$A
pc_vec <- centered_filtered_data$play_count

M <- cbind(prf_vec,pc_vec,acc_vec,ss_vec,s_vec,a_vec)
CM <- cor(M)
CM[1,]
```
We mentioned before that (intuitively) accuracy wasn't going to be a strong factor for one's performance. A correlation of 0.06 between performance and accuracy somewhat validates that. 

With the information above, we'll generate some plots. 

```{r}
# 1st experimental correlation in regards to performance, q = 0.3379438
ggplot(centered_filtered_data) +
  geom_point(aes(x = play_count, y = performance),alpha = 1/20) +
  geom_abline(slope = CM['pc_vec','prf_vec'],color = "red")

# 2nd experimental correlation in regards to performance, q = 0.1608370
ggplot(centered_filtered_data) +
  geom_point(aes(x = S, y = performance),alpha = 1/20) +
  geom_abline(slope = CM['s_vec','prf_vec'],color = "red")

# last experimental correlation in regards to performance, q = -0.05939164
ggplot(centered_filtered_data) +
  geom_point(aes(x = accuracy, y = performance),alpha = 1/20) +
  geom_abline(slope = CM['acc_vec','prf_vec'],color = "red") 

```

